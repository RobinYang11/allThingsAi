{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"robin\")\n",
    "\n",
    "# use GPU if available\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建高斯分布\n",
    "nPerClass = 1000\n",
    "imgSize = 91\n",
    "\n",
    "x = np.linspace(-4, 4, imgSize)\n",
    "X, Y = np.meshgrid(x, x)\n",
    "\n",
    "widths = [1.8, 2.4]\n",
    "\n",
    "images = torch.zeros(2*nPerClass, 1, imgSize, imgSize)\n",
    "labels = torch.zeros(2*nPerClass)\n",
    "\n",
    "for i in range(2*nPerClass):\n",
    "\n",
    "    # 创建高斯分布\n",
    "\n",
    "    # 随机中心\n",
    "    ro = 2 * np.random.randn(2)\n",
    "    # 高斯分布\n",
    "    G = np.exp(-((X-ro[0])**2 + (Y-ro[1])**2)/(2**widths[i % 2]))\n",
    "\n",
    "    G = G + np.random.randn(imgSize, imgSize)/5\n",
    "\n",
    "    images[i, :, :, :] = torch.Tensor(G).view(1, imgSize, imgSize)\n",
    "\n",
    "labels = labels[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: use scikitlearn to split the data\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    images, labels, test_size=.1)\n",
    "\n",
    "# Step 3: convert into PyTorch Datasets\n",
    "train_data = TensorDataset(train_data, train_labels)\n",
    "test_data = TensorDataset(test_data, test_labels)\n",
    "\n",
    "# Step 4: translate into dataloader objects\n",
    "batchsize = 32\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=batchsize, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=test_data.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a class for the model\n",
    "def makeTheNet(fcUnits):\n",
    "\n",
    "    class gausnet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # all layers in one go using nn.Sequential\n",
    "            self.enc = nn.Sequential(\n",
    "                # output size: (91+2*1-3)/1 + 1 = 91\n",
    "                nn.Conv2d(1, 6, 3, padding=1),\n",
    "                nn.ReLU(),                   # note that relu is treated like a \"layer\"\n",
    "                nn.AvgPool2d(2, 2),           # output size: 91/2 = 45\n",
    "                # output size: (45+2*1-3)/1 + 1 = 45\n",
    "                nn.Conv2d(6, 4, 3, padding=1),\n",
    "                nn.ReLU(),                   #\n",
    "                nn.AvgPool2d(2, 2),           # output size: 45/2 = 22\n",
    "                nn.Flatten(),                # vectorize conv output\n",
    "                nn.Linear(22*22*4, 2*fcUnits),  # output size: 2*fcUnits\n",
    "                # NOTE: in the video I forgot to include ReLU between the linear layers\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(2*fcUnits, fcUnits),  # output size: fcUnits\n",
    "                nn.ReLU(),                   #\n",
    "                nn.Linear(fcUnits, 1),        # output size: 1\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.enc(x)\n",
    "\n",
    "    # create the model instance\n",
    "    net = gausnet()\n",
    "\n",
    "    # loss function\n",
    "    lossfun = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=.001)\n",
    "\n",
    "    return net, lossfun, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# a function that trains the model\n",
    "\n",
    "def function2trainTheModel(fcUnits):\n",
    "\n",
    "    # number of epochs\n",
    "    numepochs = 10\n",
    "\n",
    "    # create a new model\n",
    "    net, lossfun, optimizer = makeTheNet(fcUnits)\n",
    "\n",
    "    # send the model to the GPU\n",
    "    net.to(device)\n",
    "\n",
    "    # initialize losses\n",
    "    trainLoss = torch.zeros(numepochs)\n",
    "    testLoss = torch.zeros(numepochs)\n",
    "    trainAcc = torch.zeros(numepochs)\n",
    "    testAcc = torch.zeros(numepochs)\n",
    "\n",
    "    # loop over epochs\n",
    "    for epochi in range(numepochs):\n",
    "\n",
    "        # loop over training data batches\n",
    "        batchLoss = []\n",
    "        batchAcc = []\n",
    "        for X, y in train_loader:\n",
    "\n",
    "            # push data to GPU\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # forward pass and loss\n",
    "            yHat = net(X)\n",
    "            loss = lossfun(yHat, y)\n",
    "\n",
    "            # backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss from this batch\n",
    "            batchLoss.append(loss.item())\n",
    "            batchAcc.append(torch.mean(((yHat > 0) == y).float()).item())\n",
    "        # end of batch loop...\n",
    "\n",
    "        # and get average losses across the batches\n",
    "        trainLoss[epochi] = np.mean(batchLoss)\n",
    "        trainAcc[epochi] = 100*np.mean(batchAcc)\n",
    "\n",
    "        # test accuracy\n",
    "        X, y = next(iter(test_loader))  # extract X,y from test dataloader\n",
    "\n",
    "        # push data to GPU\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.no_grad():  # deactivates autograd\n",
    "            yHat = net(X)\n",
    "            loss = lossfun(yHat, y)\n",
    "\n",
    "        # compare the following really long line of code to the training accuracy lines\n",
    "        testLoss[epochi] = loss.item()\n",
    "        testAcc[epochi] = 100*torch.mean(((yHat > 0) == y).float()).item()\n",
    "\n",
    "    # end epochs\n",
    "\n",
    "    # function output\n",
    "    return trainLoss, testLoss, trainAcc, testAcc, net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crazy note: this cell took ~15 minutes on CPU!\n",
    "\n",
    "# specify number of hidden units\n",
    "numberOfLinearUnits = np.round(np.linspace(5, 500, 20))\n",
    "\n",
    "# initialize results matrix\n",
    "results = np.zeros((len(numberOfLinearUnits), 4))\n",
    "\n",
    "for i, nunits in enumerate(numberOfLinearUnits):\n",
    "    trainLoss, testLoss, trainAcc, testAcc, net = function2trainTheModel(\n",
    "        int(nunits))\n",
    "    results[i, :] = [trainLoss[-1], testLoss[-1], trainAcc[-1], testAcc[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m16\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[1;32m      3\u001b[0m ax[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mplot(numberOfLinearUnits, results[:, :\u001b[39m2\u001b[39m], \u001b[39m'\u001b[39m\u001b[39ms-\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m ax[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mset_xlabel(\u001b[39m'\u001b[39m\u001b[39mNumber of units in final linear layer\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "ig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "ax[0].plot(numberOfLinearUnits, results[:, :2], 's-')\n",
    "ax[0].set_xlabel('Number of units in final linear layer')\n",
    "ax[0].set_ylabel('Loss (MSE)')\n",
    "ax[0].set_title('Final model loss')\n",
    "ax[0].legend(['Train', 'Test'])\n",
    "\n",
    "ax[1].plot(numberOfLinearUnits, results[:, 2:], 's-')\n",
    "ax[1].set_xlabel('Number of units in final linear layer')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_title('Final model test accuracy')\n",
    "ax[1].legend(['Train', 'Test'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "ax[0].plot(numberOfLinearUnits, results[:, :2], 's-')\n",
    "ax[0].set_xlabel('Number of units in final linear layer')\n",
    "ax[0].set_ylabel('Loss (MSE)')\n",
    "ax[0].set_title('Final model loss')\n",
    "ax[0].legend(['Train', 'Test'])\n",
    "\n",
    "ax[1].plot(numberOfLinearUnits, results[:, 2:], 's-')\n",
    "ax[1].set_xlabel('Number of units in final linear layer')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_title('Final model test accuracy')\n",
    "ax[1].legend(['Train', 'Test'])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
